{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist, cifar10\n",
    "(img_train, label_train), (img_test, label_test) = cifar10.load_data()\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Input, Add, Flatten, AveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import losses, optimizers\n",
    "\n",
    "row, col = img_train.shape[1], img_train.shape[2]\n",
    "\n",
    "if len(img_train.shape) == 4 :\n",
    "    depth = img_train.shape[3]\n",
    "else:\n",
    "    depth = 1\n",
    "    img_train = img_train.reshape(img_train.shape[0], row, col, depth)\n",
    "    img_test = img_test.reshape(img_test.shape[0], row, col, depth)\n",
    "\n",
    "img_train = img_train.astype('float32')\n",
    "img_test = img_test.astype('float32')\n",
    "img_train /= 255\n",
    "img_test /= 255\n",
    "\n",
    "label_train = np_utils.to_categorical(label_train, 10)\n",
    "label_test = np_utils.to_categorical(label_test, 10)\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, input_unit, filter_n, layer_n):\n",
    "        self.result = input_unit\n",
    "        self.layer_n = layer_n\n",
    "        self.filter_n = filter_n\n",
    "    def add(self, filter_n):\n",
    "        tower_1 = Conv2D(filter_n, (3, 3), padding='same')(self.result)\n",
    "        tower_1 = BatchNormalization(axis = 3)(tower_1)\n",
    "        tower_1 = Activation('relu')(tower_1)\n",
    "        tower_2 = Conv2D(filter_n, (3, 3), padding='same')(tower_1)\n",
    "        tower_2 = BatchNormalization(axis = 3)(tower_2)\n",
    "        add_1 = Add()([tower_2, self.result])\n",
    "        add_1 = Activation('relu')(add_1)\n",
    "        \n",
    "        return add_1\n",
    "    \n",
    "    def implement(self, stride=2):\n",
    "        tower_1 = Conv2D(self.filter_n, (3, 3), strides=stride, padding='same')(self.result)\n",
    "        tower_1 = BatchNormalization(axis = 3)(tower_1)\n",
    "        tower_1 = Activation('relu')(tower_1)\n",
    "        tower_2 = Conv2D(self.filter_n, (3, 3), padding='same')(tower_1)\n",
    "        tower_2 = BatchNormalization(axis = 3)(tower_2)\n",
    "        one_by_one = Conv2D(self.filter_n, (1, 1), strides=stride, padding='same')(self.result)\n",
    "        self.result = Add()([tower_2, one_by_one])\n",
    "        self.result = Activation('relu')(self.result)\n",
    "        \n",
    "        for d in range(self.layer_n - 1):            \n",
    "            self.result = self.add(self.filter_n)\n",
    "            \n",
    "        return self.result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 32, 32, 3)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 32, 32, 16)    448         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 32, 32, 16)    64          conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32, 32, 16)    0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 32, 32, 16)    2320        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 32, 32, 16)    64          conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32, 32, 16)    0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 32, 32, 16)    2320        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 32, 32, 16)    64          conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 32, 32, 16)    272         activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 32, 32, 16)    0           batch_normalization_3[0][0]      \n",
      "                                                                   conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32, 32, 16)    0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 32, 32, 16)    2320        activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, 32, 32, 16)    64          conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 32, 32, 16)    0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 32, 32, 16)    2320        activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, 32, 32, 16)    64          conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 32, 32, 16)    0           batch_normalization_5[0][0]      \n",
      "                                                                   activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 32, 32, 16)    0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 32, 32, 16)    2320        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, 32, 32, 16)    64          conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 32, 32, 16)    0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 32, 32, 16)    2320        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 32, 32, 16)    64          conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 32, 32, 16)    0           batch_normalization_7[0][0]      \n",
      "                                                                   activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 32, 32, 16)    0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 16, 16, 32)    4640        activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 16, 16, 32)    128         conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 16, 16, 32)    0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)               (None, 16, 16, 32)    9248        activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 16, 16, 32)    128         conv2d_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)               (None, 16, 16, 32)    544         activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 16, 16, 32)    0           batch_normalization_9[0][0]      \n",
      "                                                                   conv2d_11[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 16, 16, 32)    0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)               (None, 16, 16, 32)    9248        activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 16, 16, 32)    128         conv2d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 16, 16, 32)    0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)               (None, 16, 16, 32)    9248        activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 16, 16, 32)    128         conv2d_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 16, 16, 32)    0           batch_normalization_11[0][0]     \n",
      "                                                                   activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 16, 16, 32)    0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)               (None, 16, 16, 32)    9248        activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 16, 16, 32)    128         conv2d_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 16, 16, 32)    0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)               (None, 16, 16, 32)    9248        activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 16, 16, 32)    128         conv2d_15[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 16, 16, 32)    0           batch_normalization_13[0][0]     \n",
      "                                                                   activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 16, 16, 32)    0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)               (None, 8, 8, 64)      18496       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 8, 8, 64)      256         conv2d_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 8, 8, 64)      0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)               (None, 8, 8, 64)      36928       activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 8, 8, 64)      256         conv2d_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)               (None, 8, 8, 64)      2112        activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 8, 8, 64)      0           batch_normalization_15[0][0]     \n",
      "                                                                   conv2d_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 8, 8, 64)      0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, 8, 8, 64)      36928       activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 8, 8, 64)      256         conv2d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 8, 8, 64)      0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, 8, 8, 64)      36928       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, 8, 8, 64)      256         conv2d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 8, 8, 64)      0           batch_normalization_17[0][0]     \n",
      "                                                                   activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 8, 8, 64)      0           add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)               (None, 8, 8, 64)      36928       activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, 8, 8, 64)      256         conv2d_21[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 8, 8, 64)      0           batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)               (None, 8, 8, 64)      36928       activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, 8, 8, 64)      256         conv2d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 8, 8, 64)      0           batch_normalization_19[0][0]     \n",
      "                                                                   activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 8, 8, 64)      0           add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePool (None, 4, 4, 64)      0           activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1024)          0           average_pooling2d_1[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 1024)          0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           524800      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            5130        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 803,994\n",
      "Trainable params: 802,618\n",
      "Non-trainable params: 1,376\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "init_n = 16\n",
    "\n",
    "input1 = Input(shape=(row, col, depth))\n",
    "input2 = Conv2D(init_n, (3, 3), strides=1, padding='same')(input1)\n",
    "input2 = BatchNormalization(axis=3)(input2)\n",
    "input2 = Activation('relu')(input2)\n",
    "layer1 = layer(input2, init_n, 3)\n",
    "res1 = layer1.implement(1)\n",
    "layer2 = layer(res1, init_n*2, 3)\n",
    "res2 = layer2.implement()\n",
    "layer3 = layer(res2, init_n*4, 3)\n",
    "res3 = layer3.implement()\n",
    "res3 = AveragePooling2D((2, 2), padding='same')(res3)\n",
    "fc = Flatten()(res3)\n",
    "fc = Dropout(0.2)(fc)\n",
    "fc = Dense(512, activation='relu')(fc)\n",
    "fc = Dropout(0.2)(fc)\n",
    "fc = Dense(10, activation='softmax')(fc)\n",
    "model = Model(inputs=input1, outputs=fc)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 52s - loss: 1.6825 - acc: 0.3905 - val_loss: 1.6560 - val_acc: 0.4288\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 50s - loss: 1.2720 - acc: 0.5395 - val_loss: 1.1567 - val_acc: 0.5872\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 50s - loss: 1.0709 - acc: 0.6146 - val_loss: 1.2293 - val_acc: 0.5728\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.9400 - acc: 0.6644 - val_loss: 1.0774 - val_acc: 0.6268\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.8327 - acc: 0.7029 - val_loss: 1.1668 - val_acc: 0.6182\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.7480 - acc: 0.7329 - val_loss: 0.8354 - val_acc: 0.7060\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.6811 - acc: 0.7585 - val_loss: 0.9468 - val_acc: 0.6958\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.6202 - acc: 0.7792 - val_loss: 0.8466 - val_acc: 0.7208\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.5622 - acc: 0.8007 - val_loss: 0.8213 - val_acc: 0.7210\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.5140 - acc: 0.8184 - val_loss: 1.0102 - val_acc: 0.6836\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.4672 - acc: 0.8336 - val_loss: 0.7475 - val_acc: 0.7528\n",
      "Epoch 12/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.4240 - acc: 0.8475 - val_loss: 0.7836 - val_acc: 0.7484\n",
      "Epoch 13/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.3799 - acc: 0.8637 - val_loss: 0.7717 - val_acc: 0.7530\n",
      "Epoch 14/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.3448 - acc: 0.8762 - val_loss: 0.8797 - val_acc: 0.7378\n",
      "Epoch 15/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.3160 - acc: 0.8872 - val_loss: 0.9383 - val_acc: 0.7330\n",
      "Epoch 16/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.2831 - acc: 0.8989 - val_loss: 0.9377 - val_acc: 0.7374\n",
      "Epoch 17/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.2595 - acc: 0.9078 - val_loss: 0.8843 - val_acc: 0.7646\n",
      "Epoch 18/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.2314 - acc: 0.9159 - val_loss: 0.9291 - val_acc: 0.7468\n",
      "Epoch 19/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.2045 - acc: 0.9269 - val_loss: 1.0850 - val_acc: 0.7312\n",
      "Epoch 20/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.1926 - acc: 0.9306 - val_loss: 1.0356 - val_acc: 0.7390\n",
      "Epoch 21/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.1690 - acc: 0.9390 - val_loss: 0.9823 - val_acc: 0.7588\n",
      "Epoch 22/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.1571 - acc: 0.9437 - val_loss: 1.2372 - val_acc: 0.7340\n",
      "Epoch 23/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.1395 - acc: 0.9494 - val_loss: 1.1243 - val_acc: 0.7394\n",
      "Epoch 24/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.1263 - acc: 0.9556 - val_loss: 1.3669 - val_acc: 0.7350\n",
      "Epoch 25/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.1113 - acc: 0.9601 - val_loss: 1.3006 - val_acc: 0.7366\n",
      "Epoch 26/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.1038 - acc: 0.9626 - val_loss: 1.1572 - val_acc: 0.7624\n",
      "Epoch 27/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0946 - acc: 0.9667 - val_loss: 1.1923 - val_acc: 0.7608\n",
      "Epoch 28/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0848 - acc: 0.9700 - val_loss: 1.1736 - val_acc: 0.7638\n",
      "Epoch 29/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0815 - acc: 0.9710 - val_loss: 1.2000 - val_acc: 0.7640\n",
      "Epoch 30/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0738 - acc: 0.9741 - val_loss: 1.1685 - val_acc: 0.7742\n",
      "Epoch 31/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0631 - acc: 0.9782 - val_loss: 1.1886 - val_acc: 0.7692\n",
      "Epoch 32/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0624 - acc: 0.9788 - val_loss: 1.2889 - val_acc: 0.7676\n",
      "Epoch 33/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0510 - acc: 0.9822 - val_loss: 1.3984 - val_acc: 0.7486\n",
      "Epoch 34/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0515 - acc: 0.9822 - val_loss: 1.4104 - val_acc: 0.7580\n",
      "Epoch 35/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0497 - acc: 0.9831 - val_loss: 1.6141 - val_acc: 0.7286\n",
      "Epoch 36/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0406 - acc: 0.9862 - val_loss: 1.3133 - val_acc: 0.7722\n",
      "Epoch 37/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0433 - acc: 0.9854 - val_loss: 1.3567 - val_acc: 0.7642\n",
      "Epoch 38/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0274 - acc: 0.9907 - val_loss: 1.3846 - val_acc: 0.7670\n",
      "Epoch 39/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0336 - acc: 0.9886 - val_loss: 1.4277 - val_acc: 0.7668\n",
      "Epoch 40/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0335 - acc: 0.9888 - val_loss: 1.7530 - val_acc: 0.7278\n",
      "Epoch 41/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0329 - acc: 0.9889 - val_loss: 1.3731 - val_acc: 0.7728\n",
      "Epoch 42/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0274 - acc: 0.9907 - val_loss: 1.5661 - val_acc: 0.7594\n",
      "Epoch 43/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0270 - acc: 0.9911 - val_loss: 1.4606 - val_acc: 0.7674\n",
      "Epoch 44/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0246 - acc: 0.9919 - val_loss: 1.4532 - val_acc: 0.7712\n",
      "Epoch 45/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0224 - acc: 0.9928 - val_loss: 1.4782 - val_acc: 0.7606\n",
      "Epoch 46/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0257 - acc: 0.9917 - val_loss: 1.5838 - val_acc: 0.7574\n",
      "Epoch 47/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0205 - acc: 0.9932 - val_loss: 1.4500 - val_acc: 0.7738\n",
      "Epoch 48/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0182 - acc: 0.9941 - val_loss: 1.3993 - val_acc: 0.7810\n",
      "Epoch 49/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0203 - acc: 0.9932 - val_loss: 1.4670 - val_acc: 0.7748\n",
      "Epoch 50/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0187 - acc: 0.9932 - val_loss: 1.5105 - val_acc: 0.7698\n",
      "Epoch 51/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0121 - acc: 0.9962 - val_loss: 1.4631 - val_acc: 0.7824\n",
      "Epoch 52/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0136 - acc: 0.9954 - val_loss: 1.5614 - val_acc: 0.7710\n",
      "Epoch 53/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0091 - acc: 0.9971 - val_loss: 1.6157 - val_acc: 0.7696\n",
      "Epoch 54/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0095 - acc: 0.9970 - val_loss: 1.6546 - val_acc: 0.7616\n",
      "Epoch 55/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0110 - acc: 0.9965 - val_loss: 1.4562 - val_acc: 0.7790\n",
      "Epoch 56/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0098 - acc: 0.9969 - val_loss: 1.6131 - val_acc: 0.7758\n",
      "Epoch 57/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0094 - acc: 0.9970 - val_loss: 1.6450 - val_acc: 0.7690\n",
      "Epoch 58/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0088 - acc: 0.9972 - val_loss: 1.5368 - val_acc: 0.7798\n",
      "Epoch 59/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0101 - acc: 0.9965 - val_loss: 1.6130 - val_acc: 0.7782\n",
      "Epoch 60/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0105 - acc: 0.9964 - val_loss: 1.7111 - val_acc: 0.7642\n",
      "Epoch 61/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0087 - acc: 0.9972 - val_loss: 1.5888 - val_acc: 0.7798\n",
      "Epoch 62/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0060 - acc: 0.9982 - val_loss: 1.5956 - val_acc: 0.7736\n",
      "Epoch 63/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0083 - acc: 0.9973 - val_loss: 1.5483 - val_acc: 0.7842\n",
      "Epoch 64/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0069 - acc: 0.9979 - val_loss: 1.5862 - val_acc: 0.7824\n",
      "Epoch 65/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0048 - acc: 0.9986 - val_loss: 1.5166 - val_acc: 0.7866\n",
      "Epoch 66/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0059 - acc: 0.9980 - val_loss: 1.5743 - val_acc: 0.7812\n",
      "Epoch 67/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0065 - acc: 0.9978 - val_loss: 1.5947 - val_acc: 0.7834\n",
      "Epoch 68/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0060 - acc: 0.9980 - val_loss: 1.5671 - val_acc: 0.7820\n",
      "Epoch 69/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0042 - acc: 0.9988 - val_loss: 1.6031 - val_acc: 0.7820\n",
      "Epoch 70/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0040 - acc: 0.9988 - val_loss: 1.5920 - val_acc: 0.7856\n",
      "Epoch 71/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0039 - acc: 0.9987 - val_loss: 1.6147 - val_acc: 0.7824\n",
      "Epoch 72/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0035 - acc: 0.9990 - val_loss: 1.6434 - val_acc: 0.7846\n",
      "Epoch 73/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0028 - acc: 0.9993 - val_loss: 1.6085 - val_acc: 0.7824\n",
      "Epoch 74/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0042 - acc: 0.9986 - val_loss: 1.6308 - val_acc: 0.7816\n",
      "Epoch 75/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0033 - acc: 0.9990 - val_loss: 1.6072 - val_acc: 0.7890\n",
      "Epoch 76/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0033 - acc: 0.9990 - val_loss: 1.6662 - val_acc: 0.7842\n",
      "Epoch 77/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0026 - acc: 0.9993 - val_loss: 1.6035 - val_acc: 0.7842\n",
      "Epoch 78/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0025 - acc: 0.9994 - val_loss: 1.6162 - val_acc: 0.7880\n",
      "Epoch 79/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0024 - acc: 0.9993 - val_loss: 1.6638 - val_acc: 0.7818\n",
      "Epoch 80/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0033 - acc: 0.9990 - val_loss: 1.6689 - val_acc: 0.7834\n",
      "Epoch 81/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0033 - acc: 0.9990 - val_loss: 1.6728 - val_acc: 0.7832\n",
      "Epoch 82/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0031 - acc: 0.9990 - val_loss: 1.6597 - val_acc: 0.7810\n",
      "Epoch 83/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0021 - acc: 0.9994 - val_loss: 1.6517 - val_acc: 0.7828\n",
      "Epoch 84/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0026 - acc: 0.9991 - val_loss: 1.6579 - val_acc: 0.7838\n",
      "Epoch 85/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0025 - acc: 0.9993 - val_loss: 1.7084 - val_acc: 0.7780\n",
      "Epoch 86/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0025 - acc: 0.9991 - val_loss: 1.7223 - val_acc: 0.7796\n",
      "Epoch 87/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0030 - acc: 0.9991 - val_loss: 1.6754 - val_acc: 0.7824\n",
      "Epoch 88/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0020 - acc: 0.9995 - val_loss: 1.6583 - val_acc: 0.7798\n",
      "Epoch 89/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0020 - acc: 0.9995 - val_loss: 1.6827 - val_acc: 0.7860\n",
      "Epoch 90/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0017 - acc: 0.9996 - val_loss: 1.6815 - val_acc: 0.7822\n",
      "Epoch 91/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0013 - acc: 0.9997 - val_loss: 1.6783 - val_acc: 0.7818\n",
      "Epoch 92/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0018 - acc: 0.9994 - val_loss: 1.6871 - val_acc: 0.7806\n",
      "Epoch 93/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0015 - acc: 0.9996 - val_loss: 1.7087 - val_acc: 0.7820\n",
      "Epoch 94/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0017 - acc: 0.9995 - val_loss: 1.6880 - val_acc: 0.7852\n",
      "Epoch 95/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0017 - acc: 0.9995 - val_loss: 1.7166 - val_acc: 0.7800\n",
      "Epoch 96/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0012 - acc: 0.9997 - val_loss: 1.6839 - val_acc: 0.7824\n",
      "Epoch 97/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0015 - acc: 0.9996 - val_loss: 1.7046 - val_acc: 0.7846\n",
      "Epoch 98/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0011 - acc: 0.9998 - val_loss: 1.6999 - val_acc: 0.7886\n",
      "Epoch 99/100\n",
      "45000/45000 [==============================] - 50s - loss: 0.0013 - acc: 0.9996 - val_loss: 1.7239 - val_acc: 0.7812\n",
      "Epoch 100/100\n",
      "45000/45000 [==============================] - 49s - loss: 0.0016 - acc: 0.9995 - val_loss: 1.7536 - val_acc: 0.7780\n",
      "10000/10000 [==============================] - 6s     \n",
      "('\\nTest score :', 1.8593565001964569)\n",
      "('Test accuracy :', 0.76739999999999997)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "model.compile(loss=losses.categorical_crossentropy, \n",
    "              optimizer=optimizers.sgd(lr=0.01, decay=0.0001, momentum=0.9),\n",
    "             metrics=['accuracy'])\n",
    "model.fit(img_train, label_train, batch_size=128, epochs=100, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(img_test, label_test)\n",
    "print('\\nTest score :', score[0])\n",
    "print('Test accuracy :', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile a model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-af3728fe5f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nTest score :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-2.0.3-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1527\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-2.0.3-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                                check_batch_axis=True, batch_size=None):\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             raise RuntimeError('You must compile a model before '\n\u001b[0m\u001b[1;32m   1286\u001b[0m                                \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m                                'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile a model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "score = model.evaluate(img_test, label_test)\n",
    "print('\\nTest score :', score[0])\n",
    "print('Test accuracy :', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
